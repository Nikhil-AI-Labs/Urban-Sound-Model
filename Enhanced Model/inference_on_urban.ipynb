{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c83b51fe",
        "outputId": "f0928e6e-787c-499a-a14e-a18cfb3f0057"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "annotation = \"/content/drive/MyDrive/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
        "audio_dir = \"/content/drive/MyDrive/UrbanSound8K/audio\" #/content/drive/MyDrive/UrbanSound8K/audio"
      ],
      "metadata": {
        "id": "mbbVSiwZig4k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "  def __init__(self,annoation,audio_dir,transformation,target_sampling_rate,num_samples,device):\n",
        "    self.annoation = pd.read_csv(annoation)\n",
        "    self.audio_dir = audio_dir\n",
        "    self.device = device\n",
        "    self.transformation = transformation.to(self.device)\n",
        "    self.target_sampling_rate = target_sampling_rate\n",
        "    self.num_samples = num_samples\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.annoation)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    audio_sample_path = self._get_audio_sample_path(index)\n",
        "    label = self._get_audio_sample_label(index)\n",
        "    signal,sr = torchaudio.load(audio_sample_path)\n",
        "    signal = signal.to(self.device)\n",
        "    signal = self._resample_if_necessary(signal,sr)\n",
        "    signal = self._mix_down_if_necessary(signal)\n",
        "    signal = self._cut_if_necessary(signal)\n",
        "    signal = self._right_pad_if_necessary(signal)\n",
        "    signal = self.transformation(signal)\n",
        "    return signal,label\n",
        "\n",
        "  def _resample_if_necessary(self, signal, sr):\n",
        "    if sr != self.target_sampling_rate:\n",
        "        resampler = torchaudio.transforms.Resample(sr, self.target_sampling_rate)\n",
        "        # Move the resampler's internal parameters to the device\n",
        "        resampler = resampler.to(self.device)\n",
        "        signal = resampler(signal)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  def _mix_down_if_necessary(self,signal):\n",
        "    if signal.shape[0]>1:\n",
        "      signal = torch.mean(signal,dim=0,keepdim=True)\n",
        "    return signal\n",
        "\n",
        "\n",
        "  def _cut_if_necessary(self,signal):\n",
        "    if signal.shape[1] > self.num_samples:\n",
        "      signal = signal[:,:self.num_samples]\n",
        "\n",
        "    return signal\n",
        "\n",
        "  def _right_pad_if_necessary(self,signal):\n",
        "    if signal.shape[1] < self.num_samples:\n",
        "      num_missing_samples = self.num_samples - signal.shape[1]\n",
        "      last_dim_padding = (0,num_missing_samples)\n",
        "      signal = torch.nn.functional.pad(signal,last_dim_padding)\n",
        "\n",
        "    return signal\n",
        "\n",
        "  def _get_audio_sample_path(self,index):\n",
        "    fold = f\"fold{self.annoation.iloc[index,5]}\"\n",
        "    path = os.path.join(self.audio_dir,fold,self.annoation.iloc[index,0])\n",
        "    return path\n",
        "\n",
        "  def _get_audio_sample_label(self,index):\n",
        "    return self.annoation.iloc[index,6]"
      ],
      "metadata": {
        "id": "lWh4lmPmxbDZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torch.cuda import is_available\n",
        "sample_rate = 22050\n",
        "num_samples = 22050\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda\"\n",
        "else:\n",
        "  device = \"cpu\"\n",
        "\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=sample_rate,\n",
        "    n_fft=1024,\n",
        "    hop_length=512,\n",
        "    n_mels=64\n",
        ")\n",
        "\n",
        "usd = AudioDataset(annotation,audio_dir,mel_spectrogram,sample_rate,num_samples,device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8bOvhZqm5fZ",
        "outputId": "8c41b055-38b6-4747-d004-5d15c7df53ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNnetwork(nn.Module):\n",
        "  def __init__(self, in_channel):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=in_channel,\n",
        "            out_channels=16,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=16,\n",
        "            out_channels=32,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv3 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=32,\n",
        "            out_channels=64,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.conv4 = nn.Sequential(\n",
        "        nn.Conv2d(\n",
        "            in_channels=64,\n",
        "            out_channels=128,\n",
        "            kernel_size=3,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        ),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2)\n",
        "    )\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(128*5*4,256),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(256,10),\n",
        "        nn.Softmax(dim=1)\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(self.conv4(self.conv3(self.conv2(self.conv1(x)))))"
      ],
      "metadata": {
        "id": "UQzXmp1xqXqN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "cnn = CNNnetwork(1).to(device)\n",
        "summary(cnn,(1,64,44))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SprJqcjytSqi",
        "outputId": "6e8df4ab-bf20-41b6-8bf8-e73e1ae26ac3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 16, 66, 46]             160\n",
            "              ReLU-2           [-1, 16, 66, 46]               0\n",
            "         MaxPool2d-3           [-1, 16, 33, 23]               0\n",
            "            Conv2d-4           [-1, 32, 35, 25]           4,640\n",
            "              ReLU-5           [-1, 32, 35, 25]               0\n",
            "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
            "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
            "              ReLU-8           [-1, 64, 19, 14]               0\n",
            "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
            "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
            "             ReLU-11           [-1, 128, 11, 9]               0\n",
            "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
            "          Flatten-13                 [-1, 2560]               0\n",
            "           Linear-14                  [-1, 256]         655,616\n",
            "             ReLU-15                  [-1, 256]               0\n",
            "           Linear-16                   [-1, 10]           2,570\n",
            "          Softmax-17                   [-1, 10]               0\n",
            "================================================================\n",
            "Total params: 755,338\n",
            "Trainable params: 755,338\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 1.84\n",
            "Params size (MB): 2.88\n",
            "Estimated Total Size (MB): 4.73\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_dict = torch.load(\"/content/drive/MyDrive/Urban_Sound.pth\", map_location=device)\n",
        "cnn.load_state_dict(state_dict)"
      ],
      "metadata": {
        "id": "La9fJbBHz81i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ddd85b-2a3c-4efe-b201-6fa3926a2bd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_mapping = [\n",
        "    \"air_conditioner\",\n",
        "    \"car_horn\",\n",
        "    \"children_playing\",\n",
        "    'dog_bark',\n",
        "    'drilling',\n",
        "    'engine_idling',\n",
        "    'gun_shot',\n",
        "    'jackhammer',\n",
        "    'siren',\n",
        "    'street_music'\n",
        "]"
      ],
      "metadata": {
        "id": "zq2m0sGbLWjc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model,input,target,class_mapping):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    prediction = model(input)\n",
        "    predicted_index = prediction[0].argmax(0)\n",
        "    predicted_class = class_mapping[predicted_index]\n",
        "    expected = class_mapping[target]\n",
        "\n",
        "  return predicted_class,expected"
      ],
      "metadata": {
        "id": "8laPxQGFLWJw"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchcodec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bPvJHZrNMCd",
        "outputId": "e903ae68-bc01-4bcf-9c2c-dc12dfb1e708"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchcodec in /usr/local/lib/python3.12/dist-packages (0.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input,target = usd[55][0] , usd[55][1]\n",
        "input.unsqueeze_(0)\n",
        "predicted,expected = predict(cnn,input,target,class_mapping)\n",
        "print(f\"The expected output is: {expected}, and the predicted is: {predicted}\")"
      ],
      "metadata": {
        "id": "zeltk7O62b7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bd8014e-5414-44d7-d438-21b49edfd765"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The expected output is: dog_bark, and the predicted is: siren\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g1gGeHvRvMcB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}